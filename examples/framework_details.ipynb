{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bb6ea8",
   "metadata": {},
   "source": [
    "# Behind the scenes: Computing with kernels, kernel transformations and selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b29fc",
   "metadata": {},
   "source": [
    "This notebooks gives examples on how to use our implementations of base kernels, kernel transformations and selection methods. These are implemented in the `bmdal` subfolder. If you just want to use our BMDAL methods, please have a look at the notebook on using BMDAL, where we show how to use the `bmdal.algorithms.select_batch()` function to perform BMDAL.\n",
    "\n",
    "The following examples may be useful if you wish to design your own BMDAL algorithm using our components, use our implemented components for some other purpose (e.g., uncertainty quantification), or implement your own components. For additional examples of usage, you may want to look at the source code files in the `bmdal` folder, especially `bmdal/algorithms.py` and `bmdal/selection.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e8605",
   "metadata": {},
   "source": [
    "We will first change the working directory from the examples subfolder to the main folder, which is required for the imports to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522d1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')   # change directory inside the notebook to the main directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8768dd8",
   "metadata": {},
   "source": [
    "When using a GPU that supports TF32 matrix multiplication, we need to disable it to avoid numerical problems. This is automatically done when using `train.ModelTrainer` or `bmdal.algorithms.select`, but we do not use these here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb44d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1765488",
   "metadata": {},
   "source": [
    "## Feature maps and feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addac884",
   "metadata": {},
   "source": [
    "Kernels and their corresponding feature maps are central to our framework. The class `bmdal.feature_map.FeatureMap` represents both a feature map and the corresponding kernel. Sometimes, for example if the features are infinite-dimensional, it may not be able to compute the features but just the kernel values. As an input to feature maps, we use data represented by subclasses of `bmdal.feature_data.FeatureData`. The simplest type of data is `TensorFeatureData`, which simply represents a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78412b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "from bmdal_reg.bmdal.feature_data import TensorFeatureData\n",
    "import torch\n",
    "\n",
    "data = TensorFeatureData(torch.arange(10)[:, None])\n",
    "print(data.get_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4ce28",
   "metadata": {},
   "source": [
    "The first dimension of the tensor is interpreted as the batch dimension. We can index `FeatureData` objects along the batch dimension similar to torch Tensors or numpy arrays, but with a few less options and with a caveat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d484997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]])\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "print(data[2].get_tensor())\n",
    "print(data[1:3].get_tensor())\n",
    "print(data[torch.as_tensor([1, 2, 3])].get_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888eb3fd",
   "metadata": {},
   "source": [
    "Note that the tensor returned by `data[2].get_tensor()` is still a rank-2 tensor (with shape `[1, 1]` instead of `[1]`). This behavior makes our implementation much easier. Note that the implementation does not support indexing with step sizes other than $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e6eea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot handle slices with step size other than 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mget_tensor())\n",
      "File \u001b[0;32m/media/data/prog/projects/batch_active_learning/bmdal_reg/bmdal/feature_data.py:177\u001b[0m, in \u001b[0;36mFeatureData.__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs: Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mslice\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndexes\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    :param idxs: Represents the subset of samples that should be returned.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    Note that if idxs is an int, the dimension will not be collapsed.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    In other words, self[i] is equivalent to self[i:i+1].\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    :return: Returns the feature data represented by the subset of indexes in idxs.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     idxs \u001b[38;5;241m=\u001b[39m \u001b[43mIndexes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_n_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mis_all_slice():\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/media/data/prog/projects/batch_active_learning/bmdal_reg/bmdal/feature_data.py:48\u001b[0m, in \u001b[0;36mIndexes.__init__\u001b[0;34m(self, n_samples, idxs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idxs, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot handle slices with step size other than 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m n_samples)\n\u001b[1;32m     50\u001b[0m     stop \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;28;01mif\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idxs\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m n_samples)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot handle slices with step size other than 1"
     ]
    }
   ],
   "source": [
    "print(data[::-1].get_tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12da20",
   "metadata": {},
   "source": [
    "In the error message, you can see that `TensorFeatureData` uses an `Indexes` object to represent the indexing. This is an internal class that is used a lot for indexing (for example when batching operations), but we don't need to worry about it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669ab01",
   "metadata": {},
   "source": [
    "We can now define a feature map for our one-dimensional feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95d7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmdal_reg.bmdal.feature_maps import IdentityFeatureMap\n",
    "\n",
    "feature_map = IdentityFeatureMap(n_features=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd7d7f",
   "metadata": {},
   "source": [
    "The constructed feature map corresponds to the simple identity feature map $\\phi(x) = x$. A large variety of feature maps can be found in `bmdal/feature_maps.py`, for example feature maps corresponding to base kernels, and they can be combined for example using other feature maps for sum or product kernels. Feature maps have many useful functions, but these functions are conveniently used through the `Features` class, which combines feature maps and feature data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46825a8",
   "metadata": {},
   "source": [
    "## Features and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3b1f6",
   "metadata": {},
   "source": [
    "The `Features` class combines a feature map and corresponding feature data. It provides methods for the functionalities of the feature map and the feature data. In the following, we will create two `Features` objects with the same feature map, representing the same kernel on artificial train and pool data, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3963197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmdal_reg.bmdal.features import Features\n",
    "\n",
    "n_features = 1024\n",
    "n_train = 100\n",
    "n_pool = 1000\n",
    "torch.manual_seed(1234)\n",
    "train_data = TensorFeatureData(torch.randn(n_train, n_features))\n",
    "pool_data = TensorFeatureData(torch.randn(n_pool, n_features))\n",
    "feature_map = IdentityFeatureMap(n_features=n_features)\n",
    "\n",
    "train_features = Features(feature_map, train_data)\n",
    "pool_features = Features(feature_map, pool_data)\n",
    "all_features = train_features.concat_with(pool_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349044c7",
   "metadata": {},
   "source": [
    "Now, we can for example compute the kernel matrix, its diagonal, the feature matrix, or the matrix of squared kernel distances $d_k(x_i, x_j)^2 = k(x_i, x_i) + k(x_j, x_j) - 2k(x_i, x_j)$ on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052bd593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1024])\n",
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.get_kernel_matrix(train_features).shape)\n",
    "print(train_features.get_kernel_matrix_diag().shape)\n",
    "print(train_features.get_feature_matrix().shape)\n",
    "print(train_features.get_sq_dists(train_features).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86f948",
   "metadata": {},
   "source": [
    "In our algorithms, we mostly need to compute individual rows of the kernel matrix, which we can obtain using indexing as for the feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43741d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1100])\n"
     ]
    }
   ],
   "source": [
    "print(pool_features[0].get_kernel_matrix(all_features).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db84ebd",
   "metadata": {},
   "source": [
    "As for the feature data, indexing with a single index does not remove the first dimension for simplicity of implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f4284",
   "metadata": {},
   "source": [
    "Note that the computations of kernel matrices etc. are vectorized. If the vectorized computation of an entire kernel matrix (or another quantity) might cause a RAM overflow, we can compute the kernel matrix in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692beb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1100, 1100])\n"
     ]
    }
   ],
   "source": [
    "print(all_features.batched(128).get_kernel_matrix(all_features.batched(128)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245de25",
   "metadata": {},
   "source": [
    "The computation above computes $128 \\times 128$ chunks of the kernel matrix in a vectorized fashion and then assembles them to the whole kernel matrix. To get an insight into how this works, we check the type of the transformed feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0114e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bmdal_reg.bmdal.feature_data.BatchedFeatureData object at 0x7f814930b7c0>\n"
     ]
    }
   ],
   "source": [
    "print(all_features.batched(128).feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f77db",
   "metadata": {},
   "source": [
    "We see that the type of the feature data has changed from `TensorFeatureData` to `BatchedFeatureData` (which holds the `TensorFeatureData` object internally). Each `FeatureData` subclass has to provide an iterator over tuples of `(FeatureData, Indexes)` objects that are used during computation. `BatchedFeatureData` provides an iterator that proceeds in batches of the given size (except for the last batch, which might be smaller). Methods like `get_kernel_matrix()` in `FeatureMap` then use this iterator to compute the result in batches. Nonetheless, if we use `get_tensor()`, the full tensor is returned at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59298cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(torch.all(all_features.feature_data.get_tensor() == all_features.batched(128).feature_data.get_tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0413e2c",
   "metadata": {},
   "source": [
    "There are many more methods for convenience, for example the following ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869b0794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1100\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(all_features.get_n_features())\n",
    "print(len(all_features))\n",
    "print(all_features.get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade44c",
   "metadata": {},
   "source": [
    "Besides the functions shown above, `Features` objects support a variety of transformations corresponding to the kernel transformations from our paper. For example, random projections (a.k.a. sketching) can be applied. In order to apply the same random projections to both train and pool features, we create a transformation on one of the two and then apply it to both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "985cca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "n_random_projections=256\n",
    "sketch_tfm = train_features.sketch_tfm(n_features=n_random_projections)\n",
    "train_features_sketched = sketch_tfm(train_features)\n",
    "pool_features_sketched = sketch_tfm(pool_features)\n",
    "print(train_features_sketched.get_n_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897f239",
   "metadata": {},
   "source": [
    "Here, the `sketch_tfm` modifies the feature map of the features to apply a sketching matrix. The new feature map is computed during `train_features.sketch_tfm()`, and then the resulting `sketch_tfm` object simply replaces the feature map of the `Features` objects that it is applied to. Hence, it should only be applied to `Features` objects that contain the same feature map (and an analogous type of feature data). We can see that the type of feature map changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3adf49a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bmdal_reg.bmdal.feature_maps.SequentialFeatureMap object at 0x7f814930acb0>\n",
      "[<bmdal_reg.bmdal.feature_maps.IdentityFeatureMap object at 0x7f814930a1d0>]\n",
      "<bmdal_reg.bmdal.feature_maps.LinearFeatureMap object at 0x7f8210113d90>\n"
     ]
    }
   ],
   "source": [
    "print(train_features_sketched.feature_map)\n",
    "print(train_features_sketched.feature_map.tfms)\n",
    "print(train_features_sketched.feature_map.feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c3ff2",
   "metadata": {},
   "source": [
    "In order to obtain the sketched features, the `SequentialFeatureMap` first applies the `IdentityFeatureMap` (which does nothing) and then a `LinearFeatureMap`, which applies the sketching matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d3289",
   "metadata": {},
   "source": [
    "The sketched features yield similar distances as the original features (all distances are similar here since the data has been drawn from a high-dimensional normal distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b59f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0.0000, 2293.8921, 2072.6375],\n",
      "        [2293.8921,    0.0000, 2237.8413],\n",
      "        [2072.6375, 2237.8413,    0.0000]])\n",
      "tensor([[1.2207e-04, 2.1194e+03, 1.9793e+03],\n",
      "        [2.1194e+03, 0.0000e+00, 2.1576e+03],\n",
      "        [1.9793e+03, 2.1576e+03, 1.2207e-04]])\n"
     ]
    }
   ],
   "source": [
    "print(train_features[:3].get_sq_dists(train_features[:3]))\n",
    "print(train_features_sketched[:3].get_sq_dists(train_features_sketched[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9f014",
   "metadata": {},
   "source": [
    "In order to make the computation of quantities such as kernel matrices faster, we would like to apply the sketching matrix to the data once in advance. This is automatically done by the precompute transformation. We can (for the sake of example) also batch this operation and then afterwards call `.simplify()`, which assembles the batches back together such that subsequent operations will not be batched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899238d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_sketched_precomputed = train_features_sketched.batched(128).precompute().simplify()\n",
    "pool_features_sketched_precomputed = pool_features_sketched.batched(128).precompute().simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbfca7",
   "metadata": {},
   "source": [
    "Another way to achieve the same result with a transform object would be to use `bmdal.features.PrecomputeTransform(128)`. We now see that we are back with an `IdentityFeatureMap` and `TensorFeatureData` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01dbb11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bmdal_reg.bmdal.feature_maps.IdentityFeatureMap object at 0x7f814930a3e0>\n",
      "<bmdal_reg.bmdal.feature_data.TensorFeatureData object at 0x7f8149527e50>\n"
     ]
    }
   ],
   "source": [
    "print(train_features_sketched_precomputed.feature_map)\n",
    "print(train_features_sketched_precomputed.feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d743b76",
   "metadata": {},
   "source": [
    "Crucially, `precompute()` does not change resulting quantities such as feature map, kernel matrix, or squared kernel distances (up to numerical precision):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c5329f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 893.5628, -108.3986,  -57.4941],\n",
      "        [-108.3986, 1009.0555,  -88.8848],\n",
      "        [ -57.4941,  -88.8848,  970.7518]])\n",
      "tensor([[ 893.5627, -108.3986,  -57.4940],\n",
      "        [-108.3986, 1009.0555,  -88.8848],\n",
      "        [ -57.4940,  -88.8848,  970.7517]])\n"
     ]
    }
   ],
   "source": [
    "print(train_features_sketched[:3].get_kernel_matrix(train_features_sketched[:3]))\n",
    "print(train_features_sketched_precomputed[:3].get_kernel_matrix(train_features_sketched_precomputed[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cacfe",
   "metadata": {},
   "source": [
    "After precomputation, subsequent computations are faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fc4ebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for precomputed: 0.0122815s\n",
      "Time for not precomputed: 0.0356688s\n"
     ]
    }
   ],
   "source": [
    "from bmdal_reg import utils\n",
    "with utils.TimePrinter('precomputed'):\n",
    "    pool_features_sketched_precomputed.get_kernel_matrix(pool_features_sketched_precomputed)\n",
    "with utils.TimePrinter('not precomputed'):\n",
    "    pool_features_sketched.get_kernel_matrix(pool_features_sketched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9967433",
   "metadata": {},
   "source": [
    "Note that the implementation of `precompute()` depends on the feature map. For example, the `ProductFeatureMap` corresponding to a product of kernels usually does not precompute the final feature matrix since it is more time- and memory-efficient to exploit the product structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5182f34",
   "metadata": {},
   "source": [
    "There are of course more transformations corresponding to the kernel transformations in our paper, such as the posterior transformation or the acs-transformations. For an overview, we refer to `bmdal/algorithms.py` and `bmdal/features.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6393c6",
   "metadata": {},
   "source": [
    "## Implementing your own feature maps, feature data or transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820d61a",
   "metadata": {},
   "source": [
    "If you want to implement your own components, please have a look at the documentation strings in `bmdal/feature_maps.py`, `bmdal/feature_data.py`, `bmdal/features.py`. The implementations of subclasses may serve as useful examples. Please be aware that some methods like `FeatureMap.get_kernel_matrix()` should not be overridden in subclasses directly since they perform an iteration to implement batching. Instead, corresponding methods like `FeatureMap.get_kernel_matrix_impl_()` should be overridden instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cc5f9",
   "metadata": {},
   "source": [
    "## Selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828513be",
   "metadata": {},
   "source": [
    "Selection methods have a simpler interface. For example, MaxDist-TP can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09132b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([655, 228, 970, 508, 150])\n"
     ]
    }
   ],
   "source": [
    "from bmdal_reg.bmdal.selection import MaxDistSelectionMethod\n",
    "sel_method = MaxDistSelectionMethod(pool_features=pool_features, train_features=train_features, sel_with_train=True)\n",
    "new_idxs = sel_method.select(5)\n",
    "print(new_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b885c60",
   "metadata": {},
   "source": [
    "Note that selection method objects have a state that gets updated during `select()`, hence `select()` can only be used once per `SelectionMethod` object. Most implemented selection methods inherit from `bmdal.selection.IterativeSelectionMethod`, which implements the iterative selection template from Appendix D.1 in our paper. If you want to implement your own selection method, you may want to take a look at the implementations in `bmdal/selection.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmdal_reg_test2_venv",
   "language": "python",
   "name": "bmdal_reg_test2_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
